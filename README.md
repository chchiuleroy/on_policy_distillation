# on_policy_distillation
read and summarize https://thinkingmachines.ai/blog/on-policy-distillation/
# -------------------------------------------- #

On-Policy Distillation 深度解析：結合 RL 與蒸餾的最佳實踐
目錄
引言與背景
大型語言模型的訓練階段
傳統訓練方法的優缺點
On-Policy Distillation 的核心概念
技術實現細節
數學推理任務的應用
個人化助手的訓練
效率與成本分析
持續學習與災難性遺忘
深入討論與理論分析
實踐建議與未來方向

引言與背景

大型語言模型的能力來源
現代大型語言模型（LLMs）在特定領域能夠展現專家級別的性能表現。這種卓越的能力源於多種核心能力的堆疊組合：輸入感知（perception of input）、知識檢索（knowledge retrieval）、計劃選擇（plan selection）以及可靠執行（reliable execution）。要培養這些能力，需要一整套系統性的訓練方法，而這些方法可以被劃分為三個廣泛的訓練階段。
第一階段是預訓練（pre-training），這個階段教授模型一般性的能力，例如語言使用、廣泛的推理能力以及世界知識。預訓練通常在海量的文本數據上進行，讓模型學習語言的統計規律和基礎知識。
第二階段是中期訓練（mid-training），這個階段賦予模型特定領域的知識，例如程式碼、醫療資料庫或內部公司文件。中期訓練讓模型能夠專精於特定任務或領域。
第三階段是後訓練（post-training），這個階段引出目標行為，例如指令遵循、數學問題推理或對話聊天。後訓練是讓模型展現出我們期望的具體行為的關鍵階段。
小模型的優勢
一個重要的發現是：經過更強訓練的小型模型，在其訓練的專業領域中，往往能夠超越更大的通用模型。使用小型模型有許多實際優勢：它們可以部署在本地環境以滿足隱私或安全考量；可以更容易地持續訓練和更新；並且能夠節省推理成本。要充分利用這些優勢，關鍵在於為訓練的後期階段選擇正確的方法。

大型語言模型的訓練階段

預訓練階段的奠基
預訓練是語言模型訓練的第一個也是最基礎的階段。在這個階段，模型在大規模的文本語料庫上進行訓練，學習語言的基本結構、語法規則、常識知識以及各種領域的基礎資訊。預訓練階段通常需要巨大的計算資源和時間投入，但它為模型建立了堅實的基礎能力。
預訓練的目標是讓模型能夠理解和生成連貫的文本，掌握語言的統計模式，並積累廣泛的世界知識。這個階段的訓練數據通常來自互聯網上的各種公開文本，包括網頁、書籍、論文等。通過在這些多樣化的數據上訓練，模型獲得了處理各種語言任務的基本能力。
中期訓練的專業化
中期訓練是在預訓練的基礎上，針對特定領域或任務進行的專門訓練。在這個階段，模型會接觸到特定領域的專業數據，例如醫療記錄、法律文件、程式碼庫或公司內部文檔。中期訓練的目的是讓模型在特定領域獲得專家級別的知識和能力。
這個階段特別重要，因為預訓練階段獲得的知識通常是廣泛但淺層的，而許多實際應用需要深入的專業知識。例如，一個用於醫療診斷的模型需要深入理解醫學術語、疾病特徵和治療方案；一個用於程式碼生成的模型需要精通各種程式語言的語法和最佳實踐。
中期訓練面臨的一個挑戰是如何在獲得新知識的同時，保持模型在預訓練階段學到的一般能力。過度的專業化訓練可能導致模型忘記之前學到的知識，這種現象被稱為「災難性遺忘」（catastrophic forgetting）。
後訓練的行為塑造
後訓練是模型訓練的最後階段，也是最關鍵的階段之一。這個階段的目標是塑造模型的具體行為，讓它能夠按照人類的期望執行任務。後訓練方法包括監督微調（Supervised Fine-Tuning, SFT）、強化學習（Reinforcement Learning, RL）以及本文重點討論的 On-Policy Distillation。
後訓練階段需要解決的核心問題是：如何讓模型不僅擁有知識，還能以正確的方式應用這些知識。例如，一個醫療助手模型不僅需要知道醫學知識，還需要知道如何以恰當的方式與患者溝通，如何遵循醫療指導原則，如何在不確定的情況下給出謹慎的建議。

傳統訓練方法的優缺點

On-Policy 訓練：強化學習的方法
On-Policy 訓練方法從學生模型自身採樣輸出（rollouts），並為這些輸出分配某種獎勵。強化學習是 On-Policy 訓練的典型代表。以數學問題為例，我們可以通過評估每個學生模型的輸出是否解決了問題來進行 On-Policy 訓練。這種評估可以由人類完成，也可以由一個可靠得到正確答案的「老師」模型來完成。
On-Policy 訓練的優勢在於，通過在模型自身的樣本上訓練，學生模型能夠以更直接的方式學習避免錯誤。當模型在其自己生成的軌跡上訓練時，它學習到的是如何在自己可能遇到的情況中做出正確的決策。
On-Policy 訓練的劣勢是強化學習有一個主要缺點：它提供非常稀疏的反饋。無論使用多少個 token，每個訓練回合只教授固定數量的資訊位元。在上述數學問題的例子中，學生模型學到「21」是錯誤答案，並更新以避免產生它嘗試過的輸出。但是，它無法學到錯誤究竟發生在哪裡——是運算順序錯誤，還是算術本身出錯。這種反饋的稀疏性使得強化學習在許多應用中效率低下。
Off-Policy 訓練：監督微調的方法
Off-Policy 訓練依賴於來自某個外部源的目標輸出，學生模型學習模仿這些輸出。監督微調（SFT）通常用於 Off-Policy 訓練：在一個精心策劃的任務特定標註範例集上進行訓練。這些標註範例的來源可以是在手頭任務上表現良好的老師模型。
我們可以使用一種稱為蒸餾（distillation）的機制：訓練學生模型以匹配老師模型的輸出分佈。我們在老師軌跡（teacher trajectories）上訓練：包括中間思考步驟在內的完整生成 token 序列。我們可以在每一步使用老師的完整下一個 token 分佈（通常稱為「logit 蒸餾」），或者只是採樣給定的序列。實際上，採樣序列提供了對老師分佈的無偏估計，並達到相同的目標。
學生模型對序列中每個 token 的更新程度，與它自己生成該 token 的不可能程度成正比。從大型老師模型的蒸餾已被證明在訓練小型模型方面非常有效，包括遵循指令、數學和科學推理、從醫療記錄中提取臨床資訊，以及進行多輪對話等任務。用於這些和其他應用的蒸餾數據集通常是開源和公開發布的。
Off-Policy 訓練的劣勢是學生在老師經常出現的情境中學習，而不是在學生自己經常會遇到的情境中學習。這可能導致複合誤差（compounding error）：如果學生犯了一個老師從未犯過的早期錯誤，它會發現自己與訓練中觀察到的狀態越來越偏離。當我們關心學生在長序列上的表現時，這個問題變得尤其嚴重。為了避免這種偏離，學生必須學會從自己的錯誤中恢復。
Off-Policy 蒸餾觀察到的另一個問題是，學生可能學會模仿老師的風格和信心，但不一定學會其事實準確性。
用下棋來類比兩種方法
如果你正在學習下棋，On-Policy 強化學習類似於在沒有指導的情況下下棋。贏或輸一場比賽的反饋直接與你自己的下法相關，但每場比賽只接收一次反饋，並且無法告訴你哪些走法對結果貢獻最大。
Off-Policy 蒸餾類似於觀看大師下棋——你觀察到極其強大的棋步，但這些棋步是在新手玩家很少會遇到的棋盤狀態下進行的。
我們想要結合強化學習的 On-Policy 相關性與蒸餾的密集獎勵訊號。對於學習下棋，這將是一位老師對你自己的每一步棋進行評分，從「失誤」到「精彩」。對於 LLM 後訓練，這就是 On-Policy Distillation。
現代的線上國際象棋平台（如 chess.com）會用分析引擎對每一步棋進行顏色分級，將棋步標記為失誤（紅色）、錯誤（橙色）、不準確（黃色）或精彩（藍色）。這正是 On-Policy Distillation 的類比：在你自己的棋局中，每一步都獲得即時、細緻的反饋。

On-Policy Distillation 的核心概念

基本原理
On-Policy Distillation 的核心思想是從學生模型採樣軌跡，並使用高性能的老師模型對每個軌跡的每個 token 進行評分。回到上面的數學例子，On-Policy Distillation 會對解決方案的每一步進行評分，懲罰導致學生得到錯誤答案的錯誤，同時強化正確執行的步驟。
這種方法結合了兩個世界的優勢：On-Policy 訓練確保學生在其自己可能遇到的情境中學習，而密集的獎勵訊號確保學生能夠從每個 token 中學習，而不僅僅是從整個序列的最終結果中學習。
與其他方法的比較
我們可以用一個表格來總結不同方法的特性：
方法
採樣方式
獎勵訊號
監督微調 (SFT)
Off-policy
密集
強化學習 (RL)
On-policy
稀疏
On-Policy Distillation
On-policy
密集

On-Policy Distillation 是唯一同時具有 On-policy 採樣和密集獎勵訊號的方法，這使它在效率和效果上都具有顯著優勢。
理論基礎與靈感來源
這項工作的 On-Policy Distillation 從幾個重要的先前研究中汲取靈感：
DAGGER 算法（Dataset Aggregation）是一種迭代的監督微調算法，包括對學生訪問狀態的老師評估。DAGGER 的核心思想是在學生模型的軌跡上收集老師的標註，然後用這些標註訓練學生。
過程獎勵建模（Process Reward Modeling）是一種強化學習方法，對學生模型思維鏈中的每一步進行評分。這種方法提供了比傳統結果獎勵更密集的反饋。
本文擴展了 Agarwal 等人和 Qwen3 團隊的先前 On-Policy Distillation 工作。使用 Tinker 訓練 API，研究者們複製了 Qwen3 在推理基準測試上實現等效性能的結果，但成本僅為強化學習的一小部分。

技術實現細節
損失函數：Reverse KL
On-Policy Distillation 可以使用多種損失函數來評分學生的軌跡。為了簡單起見，論文選擇了每個 token 的 Reverse KL——在相同先前軌跡條件下，學生（π_θ）和老師（π_teacher）分佈之間的散度：
KL(π_θ || π_teacher) = E[log π_θ(x_{t+1} | x_{1..t}) - log π_teacher(x_{t+1} | x_{1..t})]

獎勵函數最小化 Reverse KL，這將學生推向在學生發現自己的每個狀態下近似老師的行為。當學生的行為與老師相同時，Reverse KL 為零。為了簡單起見，使用折扣因子為零：在任何給定的時間步，學生只優化即時的下一個 token，而不考慮未來的 token。雖然在數學上更正確，但實踐中發現折扣因子大於 0 並不能提高性能，因此為了簡單起見選擇零。
Reverse KL 的特性與優勢
Reverse KL 與強化學習具有自然的協同作用，強化學習通常優化獎勵模型誘導的某種形式的序列級 Reverse KL。然而，與實踐中大多數獎勵模型不同，Reverse KL 是「不可破解的」（unhackable），因為低 KL 總是對應於從老師模型角度來看的高概率理想行為。
Reverse KL 的另一個有用屬性是它是「尋找模式的」（mode seeking）——它學習一種特定的行為（老師的行為），而不是將其分佈分散到幾個次優選項上。這與 Forward KL（用於標準蒸餾）形成對比，Forward KL 是「覆蓋模式的」（mode covering），可能導致學生學習多種行為的平均。
計算效率的優勢
這種方法提供了顯著的計算節省。由於它不需要完成採樣才能計算獎勵，我們可以使用更短或部分的輸出進行訓練。查詢老師的 log 概率也只需要從較大模型進行一次前向傳播，而軌跡是由更小、更便宜的學生生成的。
我們也不需要單獨的獎勵或標註模型。將基於蒸餾的每 token 獎勵與序列級環境獎勵結合可能有優勢；這是未來研究的一個有趣領域。
實際案例說明
論文展示了一個真實的錯誤學生軌跡被老師評分的例子。這個例子來自 SimpleBench，依賴於模型做出一個關鍵觀察，即問題的前提很重要：正確答案是「B. 0」，因為冰塊會在煎鍋中融化。
學生模型 Qwen3-4B-Instruct-2507 錯誤地將此視為純數學問題，沒有考慮物理背景。較深的顏色表示從老師模型 Qwen3-235B-A22B-Instruct-2507 獲得更高懲罰的 token，老師正確解決了這個問題。
我們看到老師懲罰了那些開始引導學生走入歧途的短語的 token，直觀地對應於指導推理的重要「分叉 token」（forking tokens）。最終答案雖然錯誤，但沒有受到懲罰——在整個前面序列的條件下，它是完全可預測的。
偽代碼實現
論文在 Tinker 的 RL 腳本基礎上實現 On-Policy Distillation，該腳本已經實現了採樣、獎勵計算和策略梯度風格的訓練。實際上，實現可以是在使用 KL 正則化的 RL 實現基礎上的一行變更：只需交換正則化器模型。
實現步驟：
初始化老師客戶端：Tinker API 能夠輕鬆為不同模型創建不同的客戶端，無需擔心模型引擎的利用率。使用採樣客戶端，因為不需要通過老師模型傳播 log 概率。


採樣軌跡：與強化學習中一樣，從學生採樣輸出。在採樣期間，RL 已經計算了學生的 log 概率 log π_θ(x)，用作重要性採樣損失的一部分。


計算獎勵：使用 compute_logprobs 對採樣的軌跡查詢老師客戶端，這將返回老師對學生採樣的 token x 的 log 概率 log π_teacher(x)。然後使用這個計算 Reverse KL。


使用 RL 訓練：將每 token 的優勢設置為負 Reverse KL，並調用 RL 重要性採樣損失函數對學生模型執行訓練更新。


在下面的實驗中，研究者通常將 On-Policy Distillation 應用於已經在特定領域知識上進行中期訓練的模型。這種訓練增加了學生生成老師分佈內的 token 的概率，儘管通常還遠不足以複製老師的性能。
監督微調使用 Forward KL，為新 token 添加支持（support）。然後 Reverse KL 方法可以在初始化的支持內執行模式尋找。通常，正如我們將在個人化示例中看到的，生成相關 token 的概率開始時為零，因為學生缺乏任何相關的領域知識。
研究者使用 On-Policy Distillation 進行後訓練，並將其與訓練專家模型的這個最後關鍵階段的其他方法進行比較。

數學推理任務的應用
實驗設置
研究者使用蒸餾在 Qwen3-8B-Base 模型中訓練數學推理能力，使用 Qwen3-32B 作為老師模型。老師（Qwen3-32B）和學生（Qwen3-8B-Base）都是 Tinker 目前支持的模型，因此可以使用 Tinker cookbook 重現實驗。
Off-Policy Distillation 基線
如前所述，所有實驗都從中期訓練形式的 Off-Policy Distillation 開始——在老師生成的範例數據集上進行監督微調。用於數學推理的數據集是 OpenThoughts-3，這是一個由 QwQ-32B（類似於 Qwen3-32B 的推理模型）生成的推理提示和響應集合。
在 400k 個提示上使用完全微調訓練學生（Qwen3-8B-Base），在 AIME'24（一個數學問題基準）上達到 60% 的分數。也可以使用 LoRA 訓練，儘管在高容量數據集上訓練時它落後於完全微調。在所有情況下，我們看到性能呈對數線性增長——初始性能提升便宜，但後期提升昂貴。
當使用高批次大小運行大規模 SFT 時，觀察到 LoRA 性能較差。可以將在 400k 提示上微調的模型視為在嘗試各種後訓練方法以提高其性能之前的檢查點。我們可以比較將 AIME'24 基準分數從 60% 提高到 70% 所需的努力。
不同方法的比較
繼續 Off-Policy Distillation：默認方法是在更多提示上微調，繼續 Off-Policy Distillation 的過程。推斷對數線性趨勢，估計模型將在大約 2M 提示時在 AIME'24 上達到 70%。這種推斷需要擴展定律在不停滯的情況下成立，這並非微不足道。然而，有大規模 Off-Policy Distillation 將 8B 模型性能提高到 70% 以上的例子，例如 OpenThoughts-3 和 DeepSeek-R1-0528-Qwen3-8B。
強化學習方法：Qwen3 技術報告在類似的 SFT 初始化之上使用 17,920 GPU 小時的 RL，在基準測試上達到 67.6% 的性能。很難直接與蒸餾成本進行比較，但考慮到關於 SFT 訓練堆疊的一些合理假設，這類似於在 2M Off-Policy Distillation 提示上訓練的成本。
Qwen 團隊還報告使用 On-Policy Distillation 以 RL 成本的十分之一在 AIME'24 上達到更高的 74.4 分數，這為本研究工作提供了靈感。
On-Policy Distillation 的結果
作為 Off-Policy Distillation 或 RL 的替代方案，研究者運行了如上所述的 On-Policy Distillation。從 400k SFT 檢查點開始，On-Policy Distillation 在大約 150 步內達到 70% 的 AIME'24 分數。150 步對應於約 77K 提示；以每個提示 4 個樣本進行訓練。
計算成本的詳細分析
跨方法比較計算成本並非易事，因為訓練與採樣與 log 概率計算成本的比率根據實現而顯著變化。研究者以 FLOPs 計算成本，這對可以在 GPU 上有效並行化的方法造成了懲罰。特別是，它高估了計算 log 概率的實際成本。
從 SFT-400K 初始化開始的不同方法比較：
SFT-2M（推斷）：約 70%（推斷），成本設為基線 1×
強化學習：68%，成本約 1×
On-Policy Distillation：70%，成本效率提升 9-30×
當給定 SFT 數據集時（如 OpenThoughts-3 的例子），或者在許多訓練運行中攤銷時，研究者發現基線成本降低 9 倍。在這種情況下，我們不計算 Off-policy 訓練的老師 FLOPs，但確實計算 On-policy 的，因為必須運行老師模型來計算學生軌跡的 log 概率。由於這種計算可以在 GPU 上便宜地並行化，GPU 小時的成本降低接近 18 倍。
然而，我們經常想在沒有 Off-Policy Distillation 數據集的新任務上訓練小型模型。如果我們包括 Off-Policy Distillation 中老師模型的全部成本——即包括從老師模型採樣的額外成本——總成本降低約 30 倍。
這些結果清楚地顯示，On-Policy Distillation 在計算效率方面具有顯著優勢，特別是當我們需要從頭開始訓練一個新任務時。

個人化助手的訓練
應用場景
除了訓練小型模型在常見任務上達到高性能外，蒸餾的另一個用例是個人化。例子包括在對話和輸出格式中遵守特定語氣，或者像工具使用和成本預算這樣的能力。我們經常想將這種行為與新的領域知識結合起來訓練。
同時訓練兩者通常很困難，輕量級微調通常不足以實現這一目標，因此需要更大的中期訓練。在新知識之上學習後訓練行為需要複雜的後訓練堆疊，通常包括專有數據和獎勵模型。雖然這種方法對前沿實驗室來說是可行的，但對其他從業者來說可能很困難或成本過高。
訓練內部助手的挑戰
研究者展示 On-Policy Distillation 可以有效地用於後訓練專業化行為。這種方法也適用於持續學習或「測試時訓練」：在部署模型時更新模型而不退化基礎性能。研究者使用在內部公司文件上進行中期訓練的模型作為範例應用。
自定義模型的一個常見目標是充當助手：在某個領域擁有專家知識，此外還具有可靠的類似助手的行為。我們可能需要對每個進行單獨的訓練，特別是當專業領域無法僅從預訓練數據中學習時，或者當學習它干擾行為時。
研究者的例子是內部公司助手，有兩個要求：
領域知識：模型對領域（公司文件）有充分的了解。預訓練模型從未見過公司的內部文件，因此無論模型規模如何，都只能猜測。研究者使用內部知識召回評估（「內部 QA」）來衡量這一點。


後訓練行為：模型展現出強大的後訓練行為，即指令遵循。研究者使用常用的 IF-eval（指令遵循評估）來衡量這一點。


新知識訓練導致行為退化
研究者從 Qwen3-8B 開始，而不是基礎模型。Qwen3-8B 在助手有用技能上進行了後訓練，例如使用強化學習的指令遵循和推理。先前的研究表明，這種強化學習只訓練原始模型的小型子網絡，因此當網絡在大量數據上進一步訓練時可能很脆弱。研究者研究了這種情況發生的程度，以及如何恢復所需的行為。
為了減少這種災難性遺忘，中期訓練的一個常見方法是混入來自原始模型預訓練分佈的「背景數據」。在這種情況下，研究者無法訪問 Qwen3 的預訓練分佈。因此，他們考慮了一個更強大且更昂貴的基線：取 Tulu3 提示——一個廣泛的聊天和指令遵循數據集——並使用 Qwen3-8B 重新採樣它們，以充當聊天背景數據。
這種由 Qwen3-8B 採樣的「On-Policy」背景數據充當 Forward KL 正則化器，在整個中期訓練過程中強化模型的原始行為。研究者發現從 Qwen3-8B 採樣比從 Qwen3-32B 採樣更好地保留了整個中期訓練過程中的聊天能力，突顯了數據來源的敏感性。
然後研究者在內部文件和聊天數據的不同混合上微調 Qwen3-8B。增加文件數據的比例直接提高了模型的知識。然而，儘管混入至少 30% 的聊天數據有助於保留大部分指令遵循能力，但沒有任何權重可以維持 IF-eval 上的原始性能。
對於任何給定的混合，研究者觀察到 IF-eval 性能在微調期間退化。這損害了使用更長訓練來幫助進一步專業化模型的能力。
使用 LoRA 的嘗試
另一種常用的方法是使用 LoRA 來約束參數更新，從而減少災難性遺忘的可能性。然而，這種方法仍不足以保留 IF-eval，而且 LoRA 學習得更少。這與先前研究「LoRA 學習得更少，遺忘得也更少」的發現一致。
On-Policy Distillation 恢復後訓練行為
接下來，研究者尋求在內部文件微調後恢復指令遵循行為。這種行為最初是通過昂貴且脆弱的 RL 訓練的。相反，研究者使用早期版本的模型 Qwen3-8B 作為老師，在 Tulu3 提示上運行 On-Policy Distillation。注意，這個訓練階段與內部文件數據無關，純粹是為了恢復指令遵循而設計的。
使用模型的早期版本作為老師來「重新喚起」在微調期間丟失的能力，使得 On-Policy Distillation 對持續學習非常有前景。我們可以在新數據微調和蒸餾階段之間交替，以恢復行為，讓我們的模型隨時間學習並保持知識更新。這種階段交替方法之前已被 Cobbe 等人在 Phasic Policy Gradient 中探索過。
在 70-30 的內部文件數據和聊天數據混合上微調後，On-Policy Distillation 在不損失任何知識的情況下幾乎完全恢復了 IF-eval 的性能；研究者還觀察到聊天能力與模型在內部 QA 評估上的「知識」性能之間存在一些正向遷移。
結果摘要表：
模型
內部 QA 評估（知識）
IF-eval（聊天）
Qwen3-8B
18%
85%
+ 中期訓練 (100%)
43%
45%
+ 中期訓練 (70%)
36%
79%
+ 中期訓練 (70%) + 蒸餾
41%
83%

中期訓練後的領域特定（內部 QA 評估）和聊天（IF-eval）性能。儘管中期訓練忘記了 Qwen3-8B 的後訓練行為，但它們可以通過 On-Policy Distillation 廉價地恢復，同時保留通過中期訓練學到的額外知識。
語言模型作為獎勵模型
本質上，研究者將語言模型本身視為獎勵模型，高概率行為被獎勵。這與逆強化學習有聯繫：高概率行為對應於假設的潛在偏好模型中的有利獎勵。任何指令調整的開放權重模型都可以在這個意義上用作獎勵模型；我們只需要訪問 compute_logprobs 函數。
蒸餾作為整合行為和知識的工具也已被用於混合推理模型（Qwen3）和專家蒸餾（DeepSeek-V3.2-Exp）。正如本研究和 Chen 等人的結果所示，On-Policy 學習可以成為增強類似基於蒸餾的「模型合併」設置的關鍵工具。

效率與成本分析
密集監督大幅提高計算效率
強化學習和 On-Policy Distillation 都通過 Reverse KL 學習，修剪基礎策略中存在的動作空間。區別在於獎勵的密度。在「LoRA Without Regret」一文中，研究者從信息論的角度呈現，強化學習每個回合只教授 O(1) 位元。相比之下，蒸餾每個回合教授 O(N) 位元，其中 N 是 token 的數量。我們能否量化通過更密集的獎勵獲得的訓練效率提升？
直接比較實驗
研究者進行了一個實驗來直接比較兩者：
從 Qwen3-8B-Base 開始（無額外 SFT）
在 DeepMath 上運行 RL，匹配「LoRA Without Regret」的程序。使用 LoRA 秩為 128。結果模型成為蒸餾的老師
從 RL 訓練的模型（2）On-Policy 蒸餾回基礎模型（1）
研究者發現，蒸餾在達到老師的性能水平時大約快 7-10 倍，使用匹配的模型架構（LoRA 秩 128）。Reverse KL 降至接近零，AIME 分數在不到 10 個梯度步驟內恢復，而 RL 需要 70 步才能達到該水平。
累積而言，所需計算的減少約為 50-100 倍：
上下文長度優勢：雖然 RL 需要在大約評估上下文下訓練（為了讓策略學習上下文限制而不產生格式懲罰），蒸餾在較短的上下文長度下學習得相當好，因為在已完成採樣的軌跡和繼續的軌跡之間沒有急劇的獎勵截止。


批次大小優勢：當 SFT 初始化強時（即當老師策略在學生策略的支持範圍內時），On-Policy Distillation 在小得多的批次大小下有效工作，因為它每個回合提供了顯著更多的位元，從而減少了梯度噪聲。當這不成立時，如在「推理蒸餾」中，需要顯著更大的批次大小。


儘管通常難以用過程監督訓練強化學習模型，但這些結果表明，作為一個廣泛的方向，過程監督和密集獎勵有可能將學習效率提高一個數量級。這與 Lightman 等人在 RL 研究中的早期結果相匹配。
蒸餾可以有效重用訓練數據以提高數據效率
對於從業者來說，收集大型訓練提示數據集可能困難且耗時。因此，我們希望能夠在訓練中多次重用提示。使用 RL，在同一提示上訓練多個 epoch 通常會導致簡單地記憶最終答案，特別是對於大型模型。相比之下，On-Policy Distillation 通過最小化 Reverse KL 來學習近似老師的完整分佈，而不是記憶單一答案。這使我們能夠從同一提示訓練許多樣本。
研究者重複了上述在數學上訓練 Qwen3-8B-Base 的實驗，但現在只使用數據集中隨機選擇的單個提示。提示是：「評估極限：$\lim_{x \to \infty} \sqrt{x} \left( \sqrt[3]{x+1} - \sqrt[3]{x-1} \right)$」
研究者在這個提示上連續訓練 20 步，每步使用 256 個輸出的批次，總共 5120 個評分序列。以順序方式在同一提示上訓練多個步驟，這通常會導致過擬合。儘管這自然效率較低，但大約匹配了老師模型的性能，儘管只在單個提示上訓練。
這個結果顯示 On-Policy Distillation 的一個強大特性：它可以從有限的數據中提取更多的學習信號，這對於數據稀缺的領域特別有價值。

持續學習與災難性遺忘
On-Policy 學習作為持續學習的工具
在個人化蒸餾部分，研究者探索了 On-Policy Distillation 將專業化訓練行為重新引入模型的能力。這推廣到更廣泛的持續學習任務集，這些任務需要獲取新知識而不降低先前的能力。
先前的工作發現 On-Policy 學習（RL）比 Off-Policy 學習遺忘得更少。然而，RL 只塑造行為——它不能很好地教授新知識，因此不能滿足持續學習的需求。
在上述部分中，研究者看到 SFT（包括 Off-Policy Distillation）無法支撐持續學習，因為它會降低行為。研究者更仔細地研究這一點，並用直接的例子證明這一點。
SFT 導致性能退化的直接證據
類似於上面，研究者通過取 Tulu3 提示並以溫度 = 1.0 從 Qwen3-32B 採樣來構建數據集，沒有進一步修改。因此，這個數據集對 Qwen3-32B 的 KL 為零。「真正 On-Policy」KL=0 數據的重要性也在之前的文章「在 LLM 推理中擊敗非確定性」中探索過。
當我們在這個模型自己樣本的數據集上運行 SFT 時會發生什麼？研究者發現，任何大於零的實際學習率都會導致指令遵循評估性能的退化！
對此的一個可能解釋是，雖然 KL 散度在期望中為 0，但每個有限批次在實踐中會表現出略微不同的分佈。在這些有限批次上訓練會導致非零梯度更新，然後使更新模型的策略與其原始狀態發生偏離。隨著時間的推移，這個過程將在自己樣本上的訓練變成 Off-Policy 訓練，這會導致與 Off-Policy 訓練相同的誤差積累和長序列上的偏離。
On-Policy Distillation 的解決方案
On-Policy Distillation 始終保持 On-Policy，由於老師保持固定，學生收斂於老師的理想行為，在自我蒸餾設置中不會像 SFT 那樣退化。這使得 On-Policy Distillation 成為持續學習的非常有前景的工具。
通過使用模型的早期版本作為老師，On-Policy Distillation 可以在不忘記已學知識的情況下不斷學習新知識。這種方法可以實現真正的持續學習系統，模型可以隨時間更新而不需要從頭重新訓練。

深入討論與理論分析
RL 在語義策略空間中搜索
研究者看到 On-Policy Distillation 可以用更少的訓練步驟複製 RL 提供的學習。對這個結果的一個解釋是，與預訓練不同，RL 不會在梯度步驟本身上花費大量計算。我們應該將 RL 視為將大部分計算用於搜索——展開策略並分配信用——而不是進行更新。
通過隨機梯度下降的預訓練正在探索高維參數空間。預訓練需要大量信息，非常難以蒸餾，部分原因是參數空間對每個網絡都有些獨特。預訓練所需的梯度步驟在計算上極其昂貴且耗時。
相比之下，我們應該將 RL 視為探索語義策略空間。在每一步，RL 嘗試對它過去發現的某種策略進行小修改。它不是在參數空間中探索，而是通過運氣「偶然發現」新策略——它正在從它已經擁有的權重集合中隨機採樣。
一旦找到好的策略，蒸餾就成為學習它的捷徑：On-Policy Distillation 不需要對 RL 課程期間的中間策略建模，而只需要對學習到的最終策略建模。如果我們只對最終策略感興趣（在生產用例中很常見），我們不需要花費計算來建模所有中間策略。
科學研究的類比
考慮一個類比：在科學研究中，我們花費大量時間和資源尋找答案和探索新想法。一旦發現結果，通過自然語言表達它來教授他人要簡單得多。我們可以將這與直觀的身體技能（如運動）進行對比。它們更難教授他人，因為知識存在於只有我們自己容易理解的內在語言中（例如肌肉記憶）。運動只能通過反復練習來學習。
類似地，RL 的搜索過程就像科學發現——昂貴且不可預測，但一旦找到解決方案，可以通過蒸餾高效地傳遞。預訓練更像是學習身體技能——需要大量實踐，難以捷徑。
策略探索與結果探索的區別
值得注意的是，策略上的探索與結果上的探索微妙不同。RL 要求基礎模型一開始就有非零的成功率，因此已經「找到了結果」，但可以在 RL 過程中改進策略，使成功結果更有可能。
這解釋了為什麼 On-Policy Distillation 需要良好的初始化：如果學生模型根本無法生成正確答案，那麼蒸餾無法幫助。這就是為什麼中期訓練（SFT）階段如此重要——它確保學生至少有時可以生成正確的 token，然後 On-Policy Distillation 可以強化這些正確的行為。
密集監督的信息論優勢
從信息論的角度來看，強化學習每個訓練回合提供 O(1) 位元的信息，無論序列長度如何。這是因為 RL 只在序列末尾提供單一的獎勵信號。相比之下，On-Policy Distillation 提供 O(N) 位元的信息，其中 N 是序列中的 token 數量，因為每個 token 都獲得單獨的反饋。
這種信息密度的差異直接轉化為學習效率的差異。當序列長度為 100 個 token 時，On-Policy Distillation 提供的信息量是 RL 的 100 倍。這解釋了為什麼 On-Policy Distillation 可以在 7-10 倍更少的訓練步驟中達到相同的性能。
Mode Seeking vs Mode Covering
Reverse KL（用於 On-Policy Distillation）和 Forward KL（用於標準 SFT）之間的一個關鍵區別是它們的模式行為：
Reverse KL（mode seeking）：學生學習專注於老師分佈的單一模式。這意味著學生會選擇一種好的行為並堅持它，而不是試圖覆蓋所有可能的行為。


Forward KL（mode covering）：學生嘗試覆蓋老師分佈的所有模式。這可能導致學生學習多種行為的平均，而不是專注於最佳行為。


對於大多數應用，mode seeking 行為更可取，因為我們通常希望模型一致地產生高質量的輸出，而不是有時產生優秀的輸出，有時產生平庸的輸出。

實踐建議與未來方向
何時使用 On-Policy Distillation
基於論文的發現，On-Policy Distillation 最適合以下場景：
已有良好初始化：學生模型已經通過 SFT 或中期訓練獲得了基本能力，可以生成在老師分佈支持範圍內的 token。


需要精細化行為：目標是改進已有的能力，而不是從零開始學習全新的知識。


計算預算有限：相比 RL，On-Policy Distillation 可以節省 7-10 倍的訓練時間；相比繼續 SFT，可以節省 9-30 倍的成本。


長序列任務：對於需要長思維鏈的任務（如數學推理），密集的 token 級反饋特別有價值。


持續學習需求：需要在不忘記已學知識的情況下學習新知識。


實施的關鍵考慮因素
選擇合適的老師模型：老師應該在目標任務上表現出色，但不需要太大。研究表明，Qwen3-32B 作為 Qwen3-8B 的老師效果很好，提供了性能和成本之間的良好平衡。
批次大小和採樣策略：當初始化良好時，可以使用較小的批次大小。對於每個提示採樣多個輸出（如 4 個樣本）可以提供更豐富的學習信號。
學習率調整：On-Policy Distillation 通常可以使用比標準 SFT 更高的學習率，因為每個更新都基於學生自己的樣本，減少了分佈偏移的風險。
監控指標：關鍵指標包括 Reverse KL（應該逐漸減少到接近零）和任務特定的性能指標（如準確率）。
漸進式訓練：對於複雜任務，可以考慮從較簡單的例子開始，逐步增加難度，讓學生模型逐步接近老師的能力。
與其他技術的結合
與 LoRA 的結合：雖然 LoRA 可以減少參數更新並降低計算成本，但研究發現在大規模訓練中，完全微調通常表現更好。LoRA 可能更適合快速原型設計或資源極其有限的情況。
與 RL 的混合方法：未來的研究可以探索將基於蒸餾的每 token 獎勵與序列級環境獎勵結合起來。這可能在需要長期規劃的任務中帶來進一步的改進。
多老師蒸餾：可以使用多個專家老師模型，每個老師專精於不同的方面。學生可以從不同的老師學習不同的能力。
動態老師選擇：根據學生當前的能力水平，動態選擇適當難度的老師，實現課程學習。
數據效率的進一步提升
研究表明，On-Policy Distillation 可以從同一提示訓練多個樣本而不會過擬合。這開啟了幾個可能性：
數據增強：對於數據稀缺的領域，可以從少量高質量提示生成大量訓練樣本。
主動學習：智能選擇學生表現不佳的提示進行更多訓練，最大化學習效率。
困難樣本挖掘：識別學生與老師差異最大的樣本，優先在這些樣本上訓練。
處理分佈外的情況
當學生遇到訓練期間從未見過的情況時：
魯棒性訓練：在訓練期間包含一些分佈外的例子，教導學生如何處理不熟悉的情況。
不確定性估計：訓練學生在不確定時表達不確定性，而不是產生錯誤的置信輸出。
回退機制：在生產部署中，可以設置閾值，當學生的置信度低於閾值時，回退到老師模型或人類判斷。
評估和驗證
多維度評估：不僅評估任務性能，還要評估模型的行為質量、安全性和一致性。
分佈內與分佈外測試：確保模型不僅在訓練分佈上表現良好，也能合理處理新情況。
長期跟蹤：在持續學習設置中，長期跟蹤性能以確保沒有災難性遺忘。
A/B 測試：在生產環境中進行 A/B 測試，比較 On-Policy Distillation 模型與基線模型的實際表現。
未來研究方向
自動課程學習：開發自動確定訓練課程的方法，逐步增加任務難度，優化學習效率。
元學習應用：研究如何使用 On-Policy Distillation 進行元學習，讓模型學會如何快速適應新任務。
多模態擴展：將 On-Policy Distillation 擴展到多模態設置，例如視覺-語言模型。
理論分析：深入研究 On-Policy Distillation 的收斂性質、樣本複雜度和泛化界限。
硬體優化：開發專門針對 On-Policy Distillation 工作流優化的硬體和系統設計。
分散式訓練：研究如何在分散式設置中高效實施 On-Policy Distillation，特別是當老師和學生模型在不同機器上時。
實際部署考慮
延遲優化：雖然 On-Policy Distillation 降低了訓練成本，但需要查詢老師模型可能增加延遲。研究如何通過批處理、快取等技術優化這一過程。
成本權衡：在實際部署中，需要權衡訓練成本、老師模型查詢成本和最終的推理成本。
版本控制：在持續學習設置中，需要仔細的版本控制和回滾策略，以防模型性能意外下降。
監控和警報：實施全面的監控系統，及時發現模型性能下降或行為異常。

結論
On-Policy Distillation 代表了語言模型後訓練領域的一個重要進展。它成功地結合了強化學習的 On-Policy 優勢和知識蒸餾的密集反饋優勢，在計算效率和學習效果上都取得了顯著的改進。
核心貢獻總結
效率提升：相比傳統 RL 快 7-10 倍，相比繼續 SFT 成本降低 9-30 倍。


方法簡單：實現相對簡單，可以在現有 RL 框架基礎上用少量代碼修改完成。


效果顯著：在數學推理任務上，從 60% 提升到 70% 的準確率，僅需 150 個訓練步驟。


通用性強：適用於多種場景，包括數學推理、個人化助手、持續學習等。


理論支持：有堅實的信息論和機器學習理論基礎。


關鍵洞察
密集監督的價值：每個 token 級別的反饋比序列級別的反饋提供了指數級更多的信息，這直接轉化為學習效率的巨大提升。
On-Policy 的重要性：在學生自己的樣本上訓練避免了分佈偏移問題，使模型能夠學會從自己的錯誤中恢復。
Reverse KL 的優勢：Mode seeking 行為確保學生專注於學習單一的高質量策略，而不是多種策略的混合。
持續學習的可能性：使用早期版本作為老師，可以在學習新知識的同時保持已有能力，開啟了真正持續學習的可能。
實踐意義
對於機器學習從業者，On-Policy Distillation 提供了一個在性能、成本和複雜度之間取得良好平衡的訓練方法。它特別適合那些需要在有限預算下訓練專業化小模型的應用。

